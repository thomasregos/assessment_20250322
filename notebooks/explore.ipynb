{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Please review the dataset and start outlining the way you would go about the model-building process with the goal of predicting the probability that a player will win a foul on his possession after he receives or recovers the ball.\n",
    "\n",
    "Notes:\n",
    "- In the case of a received pass, the previous event (related event) is to be inspected for the pass attributes (location, length, highness...)\n",
    "- Aggregated player data could be useful (ratio of passes under pressure signaling if he waits for pressure, dribbles per match)\n",
    "- Referees are really important\n",
    "- Home or away\n",
    "- (Game state)\n",
    "\n",
    "\n",
    "Cooridnates\n",
    "- I suspect the origo is different for the two teams competing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Tuple\n",
    "from mplsoccer import VerticalPitch, add_image, FontManager, Sbopen, Pitch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "from hdbscan import HDBSCAN\n",
    "from hyperopt import atpe, fmin, hp\n",
    "from umap.umap_ import UMAP\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(location_col: pd.Series) -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    The main purpose of this function is to bring the location columns to usable form\n",
    "        Inputs:\n",
    "            location_col (pd.Series): Pandas series containing location data.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[pd.Series, pd.Series]: Extracted X, Y coordinates as two series.\n",
    "    \"\"\"\n",
    "    splitted_cols = location_col.str.split(',', expand=True)\n",
    "    loc_x, loc_y = splitted_cols.iloc[:, 0], splitted_cols.iloc[:, 1]\n",
    "    loc_x = pd.to_numeric(loc_x, errors='coerce')\n",
    "    loc_y = pd.to_numeric(loc_y, errors='coerce')\n",
    "    \n",
    "    return loc_x, loc_y\n",
    "\n",
    "def add_in_M3(input_df: pd.DataFrame, x_name: str='x') -> pd.Series:\n",
    "    return ((input_df[x_name]<=80) & (input_df[x_name]>=40)).astype(int)\n",
    "\n",
    "def add_in_DBox(input_df: pd.DataFrame, x_name: str='x', y_name: str='x') -> pd.Series:\n",
    "    return (((input_df[x_name]<=15) & (input_df[y_name]>=33) & (input_df[y_name]<=47) & (input_df[x_name]>=2))).astype(int)\n",
    "\n",
    "def add_forward_pass(input_df: pd.DataFrame, pass_angle_name: str='pass.angle') -> pd.Series:\n",
    "    return (input_df[pass_angle_name].between(-np.pi/4, np.pi/4)).astype(int)\n",
    "\n",
    "def plot_histograms_grid(df, category_col, bins=None):\n",
    "    # Number of columns (excluding category column)\n",
    "    num_cols = len(df.columns) - 1  # Exclude the category column\n",
    "    num_rows = int(np.ceil(num_cols / 4))  # Calculate number of rows needed for 4 columns per row\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, 4, figsize=(15, 3 * num_rows))  # Create grid\n",
    "    \n",
    "    # Flatten axes for easy indexing\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(df.drop(columns=[category_col]).columns):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Drop NaN values from the current column\n",
    "        df_col = df.dropna(subset=[col])\n",
    "        df_col = df.loc[df[col]!='NONE']\n",
    "\n",
    "        # Plot histograms for each category in the column\n",
    "        ax.hist(df_col[df_col[category_col] == 0][col],\n",
    "                bins=bins,\n",
    "                alpha=0.7, label='No foul', color='grey', density=True)\n",
    "        ax.hist(df_col[df_col[category_col] == 1][col],\n",
    "                bins=bins,\n",
    "                alpha=0.5, label='Foul won', color='red', density=True)\n",
    "        \n",
    "        ax.set_title(col)  # Set title as column name\n",
    "        ax.legend(prop={'size': 8})  # Adjust legend size\n",
    "        ax.set_yticks([])\n",
    "        ax.set_ylabel(\"\")\n",
    "    \n",
    "    # Hide empty subplots if any\n",
    "    for i in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram_one(df_visu, category_col, content_col=None, density=False, bins=None):\n",
    "    content_col = category_col if content_col is None else content_col\n",
    "    df_visu = df_visu.dropna(subset=[category_col])  # Drop NaN values from the column\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    # Plot histograms for each category\n",
    "    plt.hist(df_visu[df_visu[category_col] == 0][content_col], alpha=0.7, label='No foul', color='grey',\n",
    "             density=density, bins=bins)\n",
    "    plt.hist(df_visu[df_visu[category_col] == 1][content_col], alpha=0.5, label='Foul won', color='red',\n",
    "             density=density, bins=bins)\n",
    "\n",
    "    plt.title(content_col)\n",
    "    plt.legend(prop={'size': 8})\n",
    "    plt.yticks([])\n",
    "    plt.ylabel(\"\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_boolean_cols = [\n",
    "    'under_pressure',\n",
    "    'under_pressure_related',\n",
    "    'counterpress',\n",
    "    'contains_foulwon'\n",
    "]\n",
    "\n",
    "to_string_cols = [\n",
    "    'type.id',\n",
    "    'play_pattern.id',\n",
    "    'play_pattern.id_related',\n",
    "    'team.id',\n",
    "    'opponent_team.id',\n",
    "    # 'team.id_related',  # useful with having opponent and own team id?\n",
    "    'player.id',\n",
    "    'position.id',\n",
    "    'position.id_related',\n",
    "    'pass.body_part.id',\n",
    "    'pass.body_part.id_related',\n",
    "    'pass.type.id',\n",
    "    'pass.type.id_related',\n",
    "    'duel.outcome.id',\n",
    "    'stadium.id',\n",
    "    'referee.id',\n",
    "    'manager_id_opponent',\n",
    "    'manager_id_own',\n",
    "    'manager_countryid_opponent',\n",
    "    'manager_countryid_own',\n",
    "    'location_x_bin',\n",
    "    'location_y_bin',\n",
    "    'location_y_bin_width',\n",
    "    'pass.height.id',\n",
    "    'pass.height.id_related',\n",
    "]\n",
    "\n",
    "to_roundup_cols = [\n",
    "    'pass.length',\n",
    "    'pass.length_related',\n",
    "    # 'location_x',\n",
    "    # 'location_y'\n",
    "]\n",
    "\n",
    "# to_roundup2_cols = [\n",
    "#     'pass.angle',\n",
    "#     'pass.angle_related',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = pd.read_csv('/Users/thomasregos/Owny/Assessment/Swish_Take_Home_Assessment_Data_-_EPL/epl_event_data_15.csv',\n",
    "                    #    nrows=10000\n",
    "                       )\n",
    "\n",
    "df_matches = pd.read_csv('/Users/thomasregos/Owny/Assessment/Swish_Take_Home_Assessment_Data_-_EPL/epl_matches_15.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event['type.name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n = 1528\n",
    "\n",
    "# bbb = df_event.iloc[max(n - 10, 0): min(n + 11, len(df_event))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training dataset\n",
    "The main goal is to create a table where all the starting actions of a mini possession is listed. Mini possession is part of a possession where all the actions are taken by the same player. These events then should be ammended with a target variable which signals that a mini possession contained a foul won or not. Additional variables are later added for more info about the event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skeleton of the training dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each row should be a mini possession of the player, which starts by receiving or recovering the ball. In order to identify the mini possessions a number is assigned to a player's events inside a possession. This number is different for all of the player's mini possessions. So if he received a pass then carried the ball and passed it, it will receive a number and then if inside the possession he receives the ball again these events will have a different number. Combining these mini possession IDs with the possession ID, match ID and the player ID all the mini possession can be identified.\n",
    "\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Mini possessions have to be detected at player level\n",
    "    - First the sub possession should be calculated and then create a matchid_possession_subpossession \n",
    "- For our purpose those mini possessions are relevant that start with ball recovery or received pass \n",
    "    - Valid mini possessions are identified by type.name as we have a better control this way. For detecting ball recovery it would be an option to check if the possession_team.name has changed but there are examples of making a recovery winning a foul and the data still says that the possession is on the other team (id: 7317). As the possession mechanics are not clear I will use type.name.\n",
    "    - Sometimes a possession starts with a 'Foul Won'. I don't think we should include those as the probability of these possessions to contain a 'Foul Won' is certain.\n",
    "    - type.name == 'Ball Recovery'\n",
    "    - pass.type.name = 'Recovery'\n",
    "        - If a recovery is also a pass. These could be also relevant if the pass provokes a hand ball.\n",
    "    - type.name == 'Ball Receipt*'\n",
    "        - Beware, there is an event for receiving an incomplete pass which was already intercepted, with type.name = 'Ball Receipt*'\n",
    "        - ball_receipt.outcome.id = 9 (incomplete pass that was actually not received)\n",
    "        - These should be None as they are not real mini possessions\n",
    "    - type.name == 'Interception'\n",
    "        - Although it is not a ball recovery by name and no instruction is given to include these, but I think this action recovers the ball in open play so it satisfies our criteria\n",
    "    - type.name == 'Duel'\n",
    "        - Although it is not a ball recovery by name and no instruction is given to include these, but I think this action recovers the ball in open play so it satisfies our criteria\n",
    "    - type.name == 'Goal Keeper'\n",
    "        - Although it is not a ball recovery by name and no instruction is given to include these, but I think this action recovers the ball in open play so it satisfies our criteria\n",
    "    - type.name == 'Clearance'\n",
    "        - Although it is not a ball recovery by name and no instruction is given to include these, but I think this action recovers the ball in open play so it satisfies our criteria\n",
    "    - type.name == 'Block'\n",
    "        - Although it is not a ball recovery by name and no instruction is given to include these, but I think this action recovers the ball in open play so it satisfies our criteria\n",
    "- Getting rid of Pressure events\n",
    "    - From our perspective it is enough to know if certain events were under pressure or not and it is reflected in the 'under_pressure' column\n",
    "    - Getting rid of the individual 'Pressure' events enables us to have a chronological event chain for player possessions\n",
    "- Getting rid of 'Foul Committed' events\n",
    "    - type.name == 'Foul Committed'\n",
    "    - Foul won is enough for our purpose\n",
    "- Foul won\n",
    "    - type.name == 'Foul Won'\n",
    "    - It can happen that a player not having the ball draws the foul. (index: 646)\n",
    "    - It can happen that a player having the ball commits a foul. (index: 898)\n",
    "    - It might happen that a foul is won but the play goes on as I saw several examples of winning a foul and then having a carry event that starts at the same location. It is supported by the fact that this carry and the subsequent events are not labeled as 'play_pattern.name' = 'From Free Kick'. These could be when the referee plays advantage. As there is no info about how to consider these fouls that were won but ultimately not resulted in a freekick I will count them as a foul won. (index: 1012)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini possessions\n",
    "df_filtered = df_event.copy()\n",
    "\n",
    "# - Getting rid of unneccessary rows\n",
    "notvalid_start_events = ['Foul Committed', 'Pressure', 'Dribbled Past']\n",
    "df_filtered = df_filtered[~df_filtered['type.name'].isin(notvalid_start_events)]\n",
    "\n",
    "# - calculating mini_possessions inside possessions\n",
    "# not perfect but we will be able to get a unique identifier for each mini possession\n",
    "df_filtered['poss_mini'] = (\n",
    "    df_filtered.groupby('possession')['player.id']\n",
    "    .transform(lambda x: x.ne(x.shift()).cumsum())\n",
    ")\n",
    "\n",
    "# - adding unique identifier for mini possessions\n",
    "df_filtered['poss_mini_id'] = df_filtered[['match_id', 'player.id', 'possession', 'poss_mini']].fillna('_').apply(lambda row: '_'.join(row.apply(lambda x: str(int(x)) if isinstance(x, float) and x.is_integer() else str(x))), axis=1)\n",
    "\n",
    "\n",
    "# - identifying mini possessions that started with a ball recovery or a ball receipt\n",
    "valid_start_events = ['Ball Recovery', 'Ball Receipt*', 'Duel', 'Interception', 'Goal Keeper', 'Clearance', 'Block']\n",
    "\n",
    "df_filtered['valid_start'] = df_filtered.groupby('poss_mini_id')['type.name'].transform(\n",
    "    lambda x: 1 if x.isin(valid_start_events).any() or (df_filtered.loc[x.index, 'pass.type.name'] == 'Recovery').any() else 0\n",
    ")\n",
    "\n",
    "# - identifying mini possession that contains Foul Won\n",
    "df_filtered['contains_foulwon'] = df_filtered.groupby('poss_mini_id')['type.name'].transform(lambda x: 1 if 'Foul Won' in x.values else 0)\n",
    "\n",
    "\n",
    "# - preserving only starting events of mini possessions\n",
    "mask = ((df_filtered['valid_start']==1))\n",
    "df_filtered = df_filtered[mask]\n",
    "\n",
    "df_training_full = df_filtered.drop_duplicates(subset=['poss_mini_id'], keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional info about the events\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only use data that is available at the time of the ball recovery or pass reception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info about prior events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the type of related events\n",
    "\n",
    "Info about opponent or teammate passes might be useful.\n",
    "\n",
    "Notes:\n",
    "- Most of the related events are passes from a teammate or an opponent pass.\n",
    "- Pressures are also above 1% but this info is reflected on the starting event's record itself.\n",
    "- Opponent shots are around 2% of the whole related events, and they might be interesting for niche events, but not used at this time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_event = df_event.copy()\n",
    "df_filtered = df_training_full.copy()\n",
    "\n",
    "df_filtered_event['seconds_elapsed'] = df_filtered_event['minute'] * 60 + df_filtered_event['second']\n",
    "df_filtered['seconds_elapsed'] = df_filtered['minute'] * 60 + df_filtered['second']\n",
    "\n",
    "# Expanding related events into separate rows\n",
    "df_exploded = df_filtered.assign(related_events=df_filtered['related_events'].str.split(', ')).explode('related_events')\n",
    "\n",
    "# Merging with original data to get the related event details\n",
    "df_merged = df_exploded.merge(df_filtered_event, left_on='related_events', right_on='id', how='left', suffixes=['', '_related'])\n",
    "\n",
    "# Keeping only related events that happened before\n",
    "df_valid = df_merged[df_merged['seconds_elapsed_related'] < df_merged['seconds_elapsed']]\n",
    "\n",
    "# Differentiating between opponent and own related events\n",
    "df_valid.loc[df_valid['team.id'] != df_valid['team.id_related'], 'type.name_related'] = (df_valid['type.name_related'].astype(str) + \"_opp\")\n",
    "\n",
    "# Ratio of certain event types in the valid related events\n",
    "rel_counts = df_valid['type.name_related'].value_counts(normalize=True)\n",
    "print(rel_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Including data about related events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserving only the desired type of related events\n",
    "df_valid = df_valid[df_valid['type.name_related'].isin(['Pass', 'Pass_opp'])]\n",
    "\n",
    "# renaming important columns\n",
    "df_valid = df_valid.rename(columns={'related_events': 'relevant_related_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filtered = df_training_full.copy()\n",
    "\n",
    "df_filtered['seconds_elapsed'] = df_filtered['minute'] * 60 + df_filtered['second']\n",
    "\n",
    "# checking if there are no two relevant related events for an event\n",
    "print(df_valid['id'].nunique() == df_valid.shape[0])\n",
    "\n",
    "# merging the relevant related event info to the training dataset\n",
    "# df_filtered = df_filtered.merge(df_valid[['seconds_elapsed', 'type.name_related', 'relevant_related_id', 'id']], how='left', on='id')\n",
    "df_filtered = df_filtered.merge(df_valid[['type.name_related', 'relevant_related_id', 'id']], how='left', on='id')\n",
    "\n",
    "# merging on additional info of the relevant event from the main data\n",
    "relevant_event_cols = [\n",
    "    'location',\n",
    "    'under_pressure',\n",
    "    'play_pattern.id',\n",
    "    'team.id',\n",
    "    'position.id',\n",
    "    'pass.length',\n",
    "    'pass.angle',\n",
    "    'pass.height.id',\n",
    "    'pass.body_part.id',\n",
    "    'pass.type.id',\n",
    "    'pass.end_location',\n",
    "    ]\n",
    "\n",
    "df_filtered = df_filtered.merge(df_event[relevant_event_cols + ['id']],\n",
    "                                how='left',\n",
    "                                left_on='relevant_related_id', right_on='id',\n",
    "                                suffixes=['', '_related'])\n",
    "\n",
    "# getting dummies for 'type.name_related'\n",
    "df_filtered = pd.concat([df_filtered, pd.get_dummies(df_filtered['type.name_related'], prefix='type.name_related', dtype=int)], axis=1)  # number of categories are fixed by above filtering\n",
    "df_filtered = df_filtered.drop(columns=['type.name_related'])\n",
    "\n",
    "\n",
    "df_training_full = df_filtered.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info about the teams, the match and the referee\n",
    "\n",
    "I include info about the team's manager, the opponent team's manager, the referee and the venue.\n",
    "\n",
    "Notes:\n",
    " - Many columns could be useful but contain the same value like 'competition.country_name' or 'referee.country.id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_training_full.copy()\n",
    "df_filtered_matches = df_matches.copy()\n",
    "\n",
    "\n",
    "# - creating manager sub table\n",
    "# -- there is an error which is manually checked and corrected\n",
    "# source: https://www.soccerbase.com/teams/team.sd?team_id=2513&teamTabs=managers\n",
    "# on matchid: 3754151 the correct manager for home_team.managers: 3626, Francesco Guidolin, NA, 1955-10-03, 112, Italy\n",
    "df_filtered_matches.loc[df_filtered_matches['match_id'] == 3754151, 'home_team.managers'] = '3626, Francesco Guidolin, NA, 1955-10-03, 112, Italy'\n",
    "# on matchid: 3753988 the correct manager for away_team.managers: 3626, Francesco Guidolin, NA, 1955-10-03, 112, Italy\n",
    "df_filtered_matches.loc[df_filtered_matches['match_id'] == 3753988, 'away_team.managers'] = '3626, Francesco Guidolin, NA, 1955-10-03, 112, Italy'\n",
    "\n",
    "# -- Splitting the column into multiple parts\n",
    "exploded_managers = ['manager_id', 'manager_name', 'manager_nickname', 'manager_birthdate', 'manager_countryid', 'manager_countryname']\n",
    "df_filtered_matches[[f\"{i}_home\" for i in exploded_managers]] = df_filtered_matches['home_team.managers'].str.split(', ', expand=True)\n",
    "df_filtered_matches[[f\"{i}_away\" for i in exploded_managers]] = df_filtered_matches['away_team.managers'].str.split(', ', expand=True)\n",
    "\n",
    "\n",
    "# - aggregating \n",
    "\n",
    "exploded_managers_mini = ['manager_id', 'manager_birthdate', 'manager_countryid']\n",
    "relevant_matchinfo_cols = [\n",
    "    'home_team.home_team_id',\n",
    "    'away_team.away_team_id',\n",
    "    'stadium.id',\n",
    "    'referee.id',\n",
    "    # 'match_date'\n",
    "] + [f\"{i}_home\" for i in exploded_managers_mini] + [f\"{i}_away\" for i in exploded_managers_mini]\n",
    "\n",
    "df_filtered = df_filtered.merge(df_filtered_matches[relevant_matchinfo_cols + ['match_id']],\n",
    "                                how='left',\n",
    "                                on='match_id')\n",
    "\n",
    "# Home or away team\n",
    "df_filtered['played_at_home'] = np.where(df_filtered['team.id'] == df_filtered['home_team.home_team_id'], 1, 0)\n",
    "\n",
    "# Opponent\n",
    "df_filtered['opponent_team.id'] = np.where(df_filtered['team.id'] == df_filtered['home_team.home_team_id'], df_filtered['away_team.away_team_id'], df_filtered['home_team.home_team_id'])\n",
    "\n",
    "# Opponent Manager\n",
    "opp_manager_exploded = [f\"{i}_opponent\" for i in exploded_managers_mini]\n",
    "df_filtered[opp_manager_exploded] = np.where(df_filtered['played_at_home'].values[:, None] == 0,\n",
    "                                             df_filtered[[f\"{i}_home\" for i in exploded_managers_mini]].to_numpy(),\n",
    "                                             df_filtered[[f\"{i}_away\" for i in exploded_managers_mini]].to_numpy())\n",
    "\n",
    "# Manager\n",
    "own_manager_exploded = [f\"{i}_own\" for i in exploded_managers_mini]\n",
    "df_filtered[own_manager_exploded] = np.where(df_filtered['played_at_home'].values[:, None] == 1,\n",
    "                                             df_filtered[[f\"{i}_home\" for i in exploded_managers_mini]].to_numpy(),\n",
    "                                             df_filtered[[f\"{i}_away\" for i in exploded_managers_mini]].to_numpy())\n",
    "\n",
    "df_training_full = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "- Location is brought to usable format, and then binned to make it less noisy.\n",
    "- Widthness is calculated from the location bins.\n",
    "- Opponent event location switched to same direction as normal events, while preserving normal direction for own team related events.\n",
    "- Identifying forward passes from angles.\n",
    "- Calculate the manager's age in days to the first day of the season.\n",
    "- Checking where the fouls happen and create zone labels for the high probability areas.\n",
    "- Short passes have a higher probability to result in a foul won and also it is extremely skewed. The median for the passes that resulted in a foul is just around 6.5 and the max is around 26 which are really low numbers considering the max possibble is 120. So it makes sense to label it, but instead I will take the log of it. For related passes the distribution much more resembles the non foul winning pass length distribution, but we will see that after taking the log a difference can be seen in the distributions.\n",
    "\n",
    "Notes\n",
    "- The pitch size looks to be 120x80\n",
    "- Previous pass end location is the same as the event start location, so there is no use for 'pass.end_location_related_y', 'pass.end_location_related_x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_training_full.copy()\n",
    "\n",
    "\n",
    "# - location\n",
    "\n",
    "# -- Bringing to usable format\n",
    "df_filtered['location_x'], df_filtered['location_y'] = extract_location(df_filtered['location'])  # location\n",
    "\n",
    "# Not included to avoid None values in numerical columns\n",
    "# df_filtered['pass.end_location_x'], df_filtered['pass.end_location_y'] = extract_location(df_filtered['pass.end_location'])  # pass end\n",
    "# df_filtered['location_related_x'], df_filtered['location_related_y'] = extract_location(df_filtered['location_related'])  # related event's location\n",
    "# df_filtered['pass.end_location_related_x'], df_filtered['pass.end_location_related_y'] = extract_location(df_filtered['pass.end_location_related'])  # related event's pass end location\n",
    "# df_filtered['carry.end_location_x'], df_filtered['carry.end_location_y'] = extract_location(df_filtered['carry.end_location'])  # carry end\n",
    "\n",
    "# related event location switch but only for opponent actions\n",
    "# Not included to avoid None values in numerical columns\n",
    "# df_filtered['location_related_y'] = np.where(df_filtered['type.name_related_Pass_opp'] == 1,\n",
    "#                                              80 - df_filtered['location_related_y'],\n",
    "#                                              df_filtered['location_related_y'])\n",
    "\n",
    "# df_filtered['location_related_x'] = np.where(df_filtered['type.name_related_Pass_opp'] == 1,\n",
    "#                                              120 - df_filtered['location_related_x'],\n",
    "#                                              df_filtered['location_related_x'])\n",
    "\n",
    "# -- Binning location\n",
    "# bin edges\n",
    "x_bins = np.linspace(0, 120, 25)\n",
    "y_bins = np.linspace(0, 80, 17)\n",
    "\n",
    "# assigning bins\n",
    "location_cols = ['location',\n",
    "                #  'pass.end_location', 'location_related'  # Not included to avoid None values in numerical columns\n",
    "                 ]\n",
    "for loc_col in location_cols:\n",
    "    # bins\n",
    "    df_filtered[f'{loc_col}_x_bin'] = np.where(\n",
    "        df_filtered[f'{loc_col}_x'].isna(),\n",
    "        np.nan,\n",
    "        np.digitize(df_filtered[f'{loc_col}_x'], x_bins, right=False) - 1)\n",
    "    df_filtered[f'{loc_col}_y_bin'] = np.where(\n",
    "        df_filtered[f'{loc_col}_y'].isna(),\n",
    "        np.nan,\n",
    "        np.digitize(df_filtered[f'{loc_col}_y'], x_bins, right=False) - 1)\n",
    "\n",
    "    # width\n",
    "    df_filtered[f'{loc_col}_y_bin_width'] = np.where(\n",
    "        df_filtered[f'{loc_col}_y_bin'].isna(),\n",
    "        np.nan, \n",
    "        df_filtered[f'{loc_col}_y_bin'].apply(lambda x: max(x, 16 - x)) - 8)\n",
    "\n",
    "# -- Angle\n",
    "df_filtered['forward_pass'] = add_forward_pass(df_filtered)\n",
    "\n",
    "# flipping pass angle for opponent related passes\n",
    "df_filtered['pass.angle_related'] = np.where(\n",
    "    df_filtered['type.name_related_Pass_opp']==1,\n",
    "    (df_filtered['pass.angle_related'] + np.pi) % (2 * np.pi),\n",
    "    df_filtered['pass.angle_related'])\n",
    "\n",
    "# calculating if an opponent related pass was a backward pass from the opponent's view\n",
    "# too few, not used\n",
    "# df_filtered.loc[df_filtered['type.name_related_Pass_opp'] == 1, 'type.name_related_Pass_opp_backward'] = (\n",
    "#     df_filtered['pass.angle_related'].between(-np.pi/4, np.pi/4)\n",
    "# ).astype(int)\n",
    "# df_filtered['type.name_related_Pass_opp_backward'] = df_filtered['type.name_related_Pass_opp_backward'].fillna(0).astype(int)\n",
    "\n",
    "# - Manager age in days at time of season start\n",
    "# Formatting\n",
    "df_filtered['manager_birthdate_opponent'] = pd.to_datetime(df_filtered['manager_birthdate_opponent'])\n",
    "df_filtered['manager_birthdate_own'] = pd.to_datetime(df_filtered['manager_birthdate_own'])\n",
    "\n",
    "# Calculating the difference in days to the first day of the season\n",
    "target_date = pd.to_datetime(df_matches['match_date'].min())\n",
    "df_filtered['manager_age_opponent'] = (target_date - df_filtered['manager_birthdate_opponent']).dt.days\n",
    "df_filtered['manager_age_own'] = (target_date - df_filtered['manager_birthdate_own']).dt.days\n",
    "\n",
    "# -- Checking if it is close to high prob foul won area (Explanation in next block)\n",
    "df_filtered['loc_in_M3'] = add_in_M3(df_filtered, 'location_x')\n",
    "df_filtered['loc_in_DBox'] = add_in_DBox(df_filtered, 'location_x', 'location_y')\n",
    "\n",
    "# -- Ball reception failure\n",
    "df_filtered['ball_receiption_failed'] = np.where(df_training_full['ball_receipt.outcome.id'] == 9, 1, 0)\n",
    "\n",
    "# -- pass type will contain only 1 other than '66'\n",
    "df_filtered['pass.type.id'] = np.where(df_filtered['pass.type.id']==66, 1, 0)\n",
    "\n",
    "\n",
    "df_training_full = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where did players win the most foul?\n",
    "I think it might be useful to check where do players won the most fouls. Based on this, valid starting events that are close to these areas, might have a higher probability of winning a foul on the possession. This is something that models could learn from the starting locations but this could speed up the learning process.\n",
    "\n",
    "Findings:\n",
    "- Most fouls were won in the middle third.\n",
    "- There is a jump in density in front of the own goal. Probably after corners and free kicks there are a lot of fouls won by defenders and especially the goalkeeper.\n",
    "- These events are fairly symmetrical along the X axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_event.copy()\n",
    "\n",
    "# filtering and aggregating data for Foul Won events\n",
    "df_filtered = df_filtered.loc[df_filtered['type.name'] == 'Foul Won']\n",
    "df_filtered['location_x'], df_filtered['location_y'] = extract_location(df_filtered['location'])  # location\n",
    "\n",
    "# visualising\n",
    "pitch = Pitch(line_color='#000009', line_zorder=2)\n",
    "fig, ax = pitch.draw(figsize=(6, 8))\n",
    "kde = pitch.kdeplot(df_filtered['location_x'], df_filtered['location_y'], ax=ax,\n",
    "                    fill=True,\n",
    "                    levels=100,\n",
    "                    thresh=0,\n",
    "                    cut=4,\n",
    "                    cmap='inferno')\n",
    "\n",
    "\n",
    "# highlighting areas\n",
    "shape1 = np.array([[40, 0], [40, 80], [80, 80], [80, 0]])\n",
    "shape2 = np.array([[2, 33], [15, 33], [15, 47], [2, 47]])\n",
    "verts = [shape1, shape2]\n",
    "pitch.polygon(verts, color='white', alpha=0.4, ax=ax, zorder=4)\n",
    "\n",
    "txt = ax.text(x=92, y=75, s='Middle third', size=12, color='white', alpha=0.6,\n",
    "              va='center', ha='center')\n",
    "\n",
    "txt = ax.text(x=15, y=53, s='Defensive box', size=12, color='white', alpha=0.6,\n",
    "              va='center', ha='center')\n",
    "\n",
    "txt = ax.text(x=60, y=-5, s='Fouls Won', size=15, color='black', va='center', ha='center')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    'Unnamed: 0',\n",
    "    'index',\n",
    "    'id',\n",
    "    # 'player.id',  # maybe it would be over reliant?\n",
    "    'timestamp',\n",
    "    'second',\n",
    "    'minute',\n",
    "    'duration',  # extremely skewed\n",
    "    'possession',\n",
    "    'related_events',\n",
    "    'location',\n",
    "    'type.name',\n",
    "    'possession_team.id',\n",
    "    'possession_team.name',\n",
    "    'play_pattern.name',\n",
    "    'team.name',\n",
    "    'tactics.formation',\n",
    "    'tactics.lineup',\n",
    "    'player.name',\n",
    "    'position.name',\n",
    "    'pass.height.name',\n",
    "    'pass.body_part.name',\n",
    "    'pass.type.name',\n",
    "    'pass.outcome.name',\n",
    "    'pass.outcome.id',  # Not known info at the time of ball reception or recovery\n",
    "    'pass.receipient.id',\n",
    "    'pass.recipient.name',\n",
    "    'pass.end_location',\n",
    "    'dribble.outcome.name',\n",
    "    'ball_receipt.outcome.id',\n",
    "    'ball_receipt.outcome.name',\n",
    "    'carry.end_location',  # only nan\n",
    "    'duel.outcome.name',\n",
    "    'competition_id',\n",
    "    'match_id',\n",
    "    'poss_mini',\n",
    "    'poss_mini_id',\n",
    "    'valid_start',\n",
    "    'relevant_related_id',\n",
    "    'location_related',\n",
    "    'pass.end_location_related',  # same as location for the main event\n",
    "    'id_related',\n",
    "    'manager_id_home',\n",
    "    'manager_id_away',\n",
    "    'manager_countryid_home',\n",
    "    'manager_countryid_away',\n",
    "    'manager_birthdate_home',\n",
    "    'manager_birthdate_away',\n",
    "    'manager_birthdate_opponent',\n",
    "    'manager_birthdate_own',\n",
    "    'pass.end_location_y',\n",
    "    'pass.end_location_x',\n",
    "    'location_related_y',\n",
    "    'location_related_x',\n",
    "    # 'pass.end_location_y_bin',  # the event might come in with this\n",
    "    # 'pass.end_location_x_bin',\n",
    "    'dribble.outcome.id',  # too few (2)\n",
    "    'home_team.home_team_id',\n",
    "    'away_team.home_team_id',\n",
    "    'team.id_related',  # useful with having opponent and own team id?\n",
    "    'location_x',  # with bins in it could be not relevant\n",
    "    'location_y',  # with bins in it could be not relevant\n",
    "    'pass.angle',  # not used to avoid None inputing\n",
    "    'pass.angle_related',  # not used to avoid None inputing\n",
    "]\n",
    "\n",
    "df_training_full = df_training_full.drop(columns=cols_to_drop, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using proper format for each columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Boolean columns\n",
    "    - Original format is not 0, 1\n",
    "- ID columns\n",
    "    - These contain numbers but the models should interpret them as categories\n",
    "- Rounding up\n",
    "    - Some values are too detailed hence they are round up to 0 or 2 decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_training_full.copy()\n",
    "\n",
    "\n",
    "for col in to_boolean_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(False).astype(int)\n",
    "\n",
    "for col in to_string_cols:\n",
    "    df_filtered[col] = df_filtered[col].apply(lambda x: str(int(x)) if pd.notna(x) else np.nan)\n",
    "    # df_filtered[col] = df_filtered[col].apply(lambda x: str(int(x)) if pd.notna(x) else 'NONE')\n",
    "\n",
    "for col in to_roundup_cols:\n",
    "    df_filtered[col] = df_filtered[col].round()\n",
    "\n",
    "# for col in to_roundup2_cols:\n",
    "#     df_filtered[col] = df_filtered[col].round(2)\n",
    "\n",
    "df_training_full = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reordering columns to have contains_foulwon at the front\n",
    "columns = ['contains_foulwon'] + [col for col in df_training_full.columns if col != 'contains_foulwon']\n",
    "df_training_full = df_training_full[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_full.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_full.to_csv('/Users/thomasregos/Owny/Assessment/Swish_Take_Home_Assessment_Data_-_EPL/Output/training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv('/Users/thomasregos/Owny/Assessment/Swish_Take_Home_Assessment_Data_-_EPL/Output/training_data.csv')\n",
    "\n",
    "df_filtered = df_input.copy()\n",
    "\n",
    "for col in to_boolean_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(False).astype(int)\n",
    "\n",
    "for col in to_string_cols:\n",
    "    df_filtered[col] = df_filtered[col].apply(lambda x: str(int(x)) if pd.notna(x) else np.nan)\n",
    "    # df_filtered[col] = df_filtered[col].apply(lambda x: str(int(x)) if pd.notna(x) else 'NONE')\n",
    "\n",
    "for col in to_roundup_cols:\n",
    "    df_filtered[col] = df_filtered[col].apply(lambda x: round(x) if pd.notna(x) else 0)\n",
    "\n",
    "# for col in to_roundup2_cols:\n",
    "#     df_filtered[col] = df_filtered[col].round(2)\n",
    "\n",
    "df_input = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assess the distribution of each variable accounting for the fact that the event was followed by a won foul or not.\n",
    "\n",
    "Most of the variables that we eneded up with have a difference visually. However, there are some, where the difference seems to be negligible. Later I will pay extra attention to some of these when assessing their importance in the models. 'pass.type.id' contained '66' as valid value and only one time a different one so I converted is to be a bool column.\n",
    "\n",
    "There are several variables that back intuition:\n",
    "- If the event is under pressure than the probability is higher to win a foul\n",
    "- There are less fouls won in the opening minutes of the games compared to no foul mini possessions\n",
    "- There is a higher probability of winning a foul in the middle third\n",
    "- If the event is a pass and it is with the right foot there is a higher probability of resulting in a foul won than for example with a pass that was a header. The intuition behind this is that a pass with the (usually) stronger foot is more precise, more dangerous hence has a higher chance to provoke a foul than a header.\n",
    "- There are teams that win less fouls than expected based on the number of their mini possessions. The two teams that dive the less or the referess dislike the most is Norwich and Liverpool.\n",
    "- If a player won a duel it is highly likely that he will be fouled (duel.outcome.id == 4)\n",
    "\n",
    "Interestingly there is more possibility to win a foul outside the defensive box than inside. I suspect that the local maximum can still be a good indicator even tough globally that probability is not higher than for example in the middle third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visu = df_input.copy()\n",
    "\n",
    "for col in to_string_cols:\n",
    "    df_visu[col] = df_visu[col].apply(lambda x: str(int(x)) if pd.notna(x) else 'NONE')\n",
    "\n",
    "plot_histograms_grid(df_visu, 'contains_foulwon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass.type.id\n",
    "\n",
    "df_filtered = df_input.copy()\n",
    "\n",
    "pass_typids = df_filtered['pass.type.id'].value_counts()\n",
    "pass_typeid_66 = df_filtered[df_filtered['pass.type.id'] == '66']['contains_foulwon'].value_counts()\n",
    "\n",
    "print(f\"Only one event contains pass.type.id == 61: {pass_typids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teams winning less fouls then expected\n",
    "\n",
    "df_filtered = df_event.copy()\n",
    "df_teams = df_filtered.groupby(['team.id'], as_index=False)[['team.name']].first()\n",
    "df_teams['team.id'] = df_teams['team.id'].astype(str)\n",
    "\n",
    "df_filtered = df_input.copy()\n",
    "\n",
    "df_filtered = df_filtered.groupby(['team.id'], as_index=False).agg(\n",
    "    foulwon_ratio = ('contains_foulwon', lambda x: x.sum() / x.count())\n",
    ").sort_values('foulwon_ratio')\n",
    "\n",
    "df_filtered = df_filtered.merge(df_teams, on='team.id')\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix\n",
    "\n",
    "Fortunately there seems to be not too much correlation between the variables.\n",
    "\n",
    "- Seconds elapsed and period naturally correlates a lot\n",
    "- Forward pass, pass height and length also correlates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_input.copy()\n",
    "num_cols = df_input.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "df_filtered = df_filtered[num_cols]\n",
    "\n",
    "df_filtered = df_filtered.corr()\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "ax = sns.heatmap(df_filtered,\n",
    "        xticklabels=df_filtered.columns,\n",
    "        yticklabels=df_filtered.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log and standardizing\n",
    "\n",
    "- Standardizing is applied to the numerical variables to bring them closer in terms of scale.\n",
    "\n",
    "- As earlier was mentioned the pass length is heavily skewed so taking the log of it makes it more like a gaussian distribution which is better to use for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_input.copy()\n",
    "df_filtered = df_filtered[df_filtered['pass.length']!=0]\n",
    "df_filtered = df_filtered[['contains_foulwon', 'pass.length']]\n",
    "df_filtered['pass.length_log'] = np.log(df_filtered['pass.length'] + 1)\n",
    "\n",
    "plot_histograms_grid(df_filtered, 'contains_foulwon', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_input.copy()\n",
    "df_filtered = df_filtered[df_filtered['pass.length_related']!=0]\n",
    "df_filtered = df_filtered[['contains_foulwon', 'pass.length_related']]\n",
    "df_filtered['pass.length_related_log'] = np.log(df_filtered['pass.length_related'] + 1)\n",
    "\n",
    "plot_histograms_grid(df_filtered, 'contains_foulwon', bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filtered = df_input.copy()\n",
    "\n",
    "# Stand\n",
    "cols_to_standardize = [\n",
    "    'seconds_elapsed',\n",
    "    'manager_age_opponent',\n",
    "    'manager_age_own'\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "df_filtered[cols_to_standardize] = scaler.fit_transform(df_filtered[cols_to_standardize])\n",
    "\n",
    "# Log\n",
    "cols_to_log = [\n",
    "    'pass.length',\n",
    "    'pass.length_related',\n",
    "]\n",
    "for col in cols_to_log:\n",
    "    df_filtered[col] = np.log(df_filtered[col] + 1)\n",
    "\n",
    "df_input = df_filtered.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dependent variable is a boolean, which represents if during the mini possession there was a foul won by the player or not. As winning a foul is quite rare the data is heavily imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_input.copy()\n",
    "foulwon_ratio = df_filtered.loc[df_filtered['contains_foulwon'] == 1]['contains_foulwon'].sum() / len(df_filtered['contains_foulwon'])\n",
    "print(f\"Only {round(foulwon_ratio, 4)*100}% of the mini possession ends up with a foul won.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered = df_input.copy()\n",
    "df_filtered = df_filtered[['contains_foulwon']]\n",
    "plot_histogram_one(df_filtered, 'contains_foulwon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_input.copy()\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df_filtered[df_filtered['contains_foulwon'] == 0]\n",
    "df_minority = df_filtered[df_filtered['contains_foulwon'] == 1]\n",
    "\n",
    "# Oversample minority class by duplication\n",
    "df_minority_oversampled = resample(df_minority, replace=True, n_samples=round(len(df_filtered)*0.1), random_state=42)\n",
    "\n",
    "# Combine the datasets\n",
    "df_filtered = pd.concat([df_majority, df_minority_oversampled])\n",
    "\n",
    "\n",
    "# df_input = df_filtered.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'type.id',\n",
    "    'play_pattern.id',\n",
    "    'play_pattern.id_related',\n",
    "    'team.id',\n",
    "    'opponent_team.id',\n",
    "    # 'player.id',\n",
    "    'position.id',\n",
    "    'position.id_related',\n",
    "    'pass.body_part.id',\n",
    "    'pass.body_part.id_related',\n",
    "    'pass.type.id',\n",
    "    'pass.type.id_related',\n",
    "    'duel.outcome.id',\n",
    "    'stadium.id',\n",
    "    'referee.id',\n",
    "    'manager_id_opponent',\n",
    "    'manager_id_own',\n",
    "    'manager_countryid_opponent',\n",
    "    'manager_countryid_own',\n",
    "    'location_x_bin',\n",
    "    'location_y_bin',\n",
    "    'location_y_bin_width',\n",
    "    'under_pressure',\n",
    "    'under_pressure_related',\n",
    "    'counterpress',\n",
    "    'pass.height.id',\n",
    "    'pass.height.id_related'\n",
    "]\n",
    "\n",
    "numerical_cols = [\n",
    "    'pass.length',\n",
    "    'pass.length_related',\n",
    "\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_filtered = df_input.copy()\n",
    "\n",
    "df_filtered = df_filtered.drop(columns='contains_foulwon')\n",
    "\n",
    "df_filtered = resample(df_filtered, replace=False, n_samples=round(len(df_filtered)*0.2), random_state=42)\n",
    "\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "OHEncoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Fit on multiple columns\n",
    "df_filtered_encoded = OHEncoder.fit_transform(df_filtered[categorical_cols])\n",
    "df_filtered_encoded = pd.DataFrame(df_filtered_encoded, columns=OHEncoder.get_feature_names_out(categorical_cols))\n",
    "df_filtered = df_filtered.drop(columns=[col for col in df_filtered.columns if 'nan' in col])\n",
    "\n",
    "df_filtered = df_filtered[numerical_cols]\n",
    "df_filtered = pd.concat([df_filtered, df_filtered_encoded], axis=1)\n",
    "\n",
    "df_filtered = df_filtered.fillna(0)\n",
    "\n",
    "df_filtered_encoded = df_filtered.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered_encoded.copy()\n",
    "\n",
    "\n",
    "# Fitting UMAP on the smaller sample\n",
    "df_sample = resample(df_filtered, replace=False, n_samples=round(len(df_filtered)*0.25), random_state=42)\n",
    "reducer = UMAP(random_state=42,\n",
    "               n_neighbors=100,\n",
    "               min_dist=0.8,\n",
    "               metric='euclidean'\n",
    "               )\n",
    "reducer.fit(df_sample)\n",
    "\n",
    "print('Fitting has finished')\n",
    "\n",
    "# Transforming all the samples\n",
    "df_reduced = pd.DataFrame(reducer.transform(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clustering = HDBSCAN(gen_min_span_tree=True,\n",
    "                     min_cluster_size=1000,\n",
    "                     min_samples=80,\n",
    "                     cluster_selection_epsilon=0.1,\n",
    "                     metric='euclidean'\n",
    "                     ).fit(df_reduced)\n",
    "df_reduced[\"CLUSTER\"] = abs(clustering.labels_)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(df_reduced[0], df_reduced[1],\n",
    "                      c=df_reduced['CLUSTER'],\n",
    "                      cmap='viridis')\n",
    "# plt.colorbar(scatter)  # Show the color bar\n",
    "plt.legend()\n",
    "plt.title('UMAP projection colored by CLUSTER')\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clustering = HDBSCAN(gen_min_span_tree=True,\n",
    "                     min_cluster_size=1000,\n",
    "                     min_samples=100,\n",
    "                     cluster_selection_epsilon=0.1,\n",
    "                     metric='euclidean'\n",
    "                     ).fit(df_reduced)\n",
    "df_reduced[\"CLUSTER\"] = abs(clustering.labels_)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(df_reduced[0], df_reduced[1],\n",
    "                      c=df_reduced['CLUSTER'],\n",
    "                      cmap='viridis')\n",
    "# plt.colorbar(scatter)  # Show the color bar\n",
    "plt.legend()\n",
    "plt.title('UMAP projection colored by CLUSTER')\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing class imbalance\n",
    "\n",
    "What techniques are not used:\n",
    "- SMOTE: I don't really want to use it as many of the columns contain None values. For the categorical ones most of the cases there is a dominant class and this way we would loose weight on the interesting ones if we input the most frequent class or we would make too much noise by random (or quasi random) inputing. For the numerical values assigning 0 would give them a meaning which might be misleading.\n",
    "\n",
    "What techniques are used:\n",
    "- Resampling the minority class\n",
    "- Undersampling the majority class\n",
    "- Using log loss (cross entropy) as evaluation metric\n",
    "- Threshold tuning to be less than 0.5\n",
    "- Class weighting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Resampling\n",
    "\n",
    "normal resampling\n",
    "\n",
    "\n",
    "#### 2. Class Weighting\n",
    "Instead of modifying the dataset, you can modify the learning process by assigning higher weights to the minority class.\n",
    "\n",
    "Many machine learning models, like logistic regression, random forests, and SVM, have a class_weight parameter (balanced option in sklearn).\n",
    "This helps the model focus more on the minority class without artificially altering the dataset.\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Using Proper Evaluation Metrics\n",
    "Accuracy is misleading in imbalanced datasets. Instead, use:\n",
    "\n",
    "Precision, Recall, and F1-score (better indicators of performance)\n",
    "ROC-AUC or PR-AUC (Precision-Recall AUC) (better for imbalance)\n",
    "Cohens Kappa or Matthews Correlation Coefficient (MCC) (helpful in skewed data)\n",
    "\n",
    "\n",
    "#### 5. Threshold Tuning\n",
    "Most models output a probability rather than a hard 0/1 decision.\n",
    "\n",
    "By adjusting the decision threshold, you can favor recall over precision or vice versa.\n",
    "For example, instead of using 0.5, lower the threshold (e.g., 0.2) to catch more positives.\n",
    "\n",
    "\n",
    "#### 7. Use Specialized Models\n",
    "Some algorithms handle imbalance better:\n",
    "\n",
    "XGBoost / LightGBM / CatBoost: Have built-in parameters for imbalance (scale_pos_weight).\n",
    "Bagging Classifier or Balanced Random Forests: Work well for imbalanced problems.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrices(true_vals_1, predicted_vals_1, true_vals_2, predicted_vals_2, labels=(\"Model 1\", \"Model 2\")):\n",
    "    \"\"\"\n",
    "    Plots two confusion matrices side by side.\n",
    "\n",
    "    Parameters:\n",
    "    - true_vals_1, predicted_vals_1: Ground truth and predictions for the first model.\n",
    "    - true_vals_2, predicted_vals_2: Ground truth and predictions for the second model.\n",
    "    - labels: Tuple of titles for each confusion matrix.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "    # First confusion matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(true_vals_1, predicted_vals_1, \n",
    "                                            normalize='true', values_format=\".0%\", ax=axes[0])\n",
    "    axes[0].set_title(labels[0])\n",
    "\n",
    "    # Second confusion matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(true_vals_2, predicted_vals_2, \n",
    "                                            normalize='true', values_format=\".0%\", ax=axes[1])\n",
    "    axes[1].set_title(labels[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_main = df_filtered.drop(columns=['position'])\n",
    "y_main = df_filtered['position']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_main, y_main, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "score_train = log_loss(y_true, y_pred_probs)\n",
    "score_test = log_loss(y_test, y_pred_probs)\n",
    "\n",
    "plot_confusion_matrices(true_vals_1, predicted_vals_1, true_vals_2, predicted_vals_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes MAGYARUL\n",
    "\n",
    "- Arra figyelj, hogy az ID-kat ne szmknt rtelmezze\n",
    "- Figyelj arra, hogy ahol lehet ne hagy None-t, pl categorical-hoz mehet valami semleges category"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assessment-20250322-0DgKILvd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
